{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "from typing import Optional, Tuple\n",
    "from einops import rearrange\n",
    "import math\n",
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rope_cache(\n",
    "    seq_len: int, n_elem: int, dtype: torch.dtype = torch.float32, device: torch.device = 'cpu', base: int = 10000\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Enhanced Transformer with Rotary Position Embedding.\n",
    "\n",
    "    Derived from: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/\n",
    "    transformers/rope/__init__.py. MIT License:\n",
    "    https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/license.\n",
    "    \"\"\"\n",
    "    # $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n",
    "    theta = 1.0 / (base ** (torch.arange(0, n_elem, 2, dtype=dtype, device=device) / n_elem))\n",
    "\n",
    "    # Create position indexes `[0, 1, ..., seq_len - 1]`\n",
    "    seq_idx = torch.arange(seq_len, dtype=dtype, device=device)\n",
    "\n",
    "    # Calculate the product of position index and $\\theta_i$\n",
    "    idx_theta = torch.outer(seq_idx, theta).float()\n",
    "\n",
    "    cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)\n",
    "\n",
    "    # this is to mimic the behaviour of complex32, else we will get different results\n",
    "    if dtype in (torch.float16, torch.bfloat16, torch.int8):\n",
    "        cache = cache.half()\n",
    "    return cache\n",
    "\n",
    "\n",
    "def apply_rope(x: torch.Tensor, rope_cache: torch.Tensor) -> torch.Tensor:\n",
    "    # truncate to support variable sizes\n",
    "    T = x.size(1)\n",
    "    rope_cache = rope_cache[:T]\n",
    "\n",
    "    # cast because the reference does\n",
    "    x_pass = x[:, :, :, 32:]\n",
    "    x_rot = x[:, :, :, :32]\n",
    "    x1, x2 = x_rot.chunk(2, dim=-1)\n",
    "    rope_cache = rope_cache.unsqueeze(1)\n",
    "\n",
    "    cos = rope_cache[..., 0]\n",
    "    sin = rope_cache[..., 1]\n",
    "\n",
    "    x_out2 = torch.cat(\n",
    "        [\n",
    "            x1 * cos - x2 * sin,\n",
    "            x2 * cos + x1 * sin,\n",
    "        ],\n",
    "        -1,\n",
    "    )\n",
    "\n",
    "    return torch.cat((x_out2, x_pass), dim=-1).type_as(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class NewGELUActivation(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n",
    "    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, 4 * hidden_size, bias=True)\n",
    "        self.linear2 = nn.Linear(4 * hidden_size, hidden_size, bias=True)\n",
    "        self.gelu = NewGELUActivation()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "class KVCache(nn.Module):\n",
    "    def __init__(self, max_batch_size, max_seq_len, hidden_size, n_heads, device='cpu', dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        cache_shape = (max_batch_size, n_heads, max_seq_len, hidden_size // n_heads)\n",
    "        self.k = torch.zeros(cache_shape, device=device, dtype=dtype)\n",
    "        self.v = torch.zeros(cache_shape, device=device, dtype=dtype)\n",
    "\n",
    "    def update(self, k_val, v_val, index):\n",
    "        assert index.shape[0] == k_val.shape[2], \"index shape {} does not match k_val shape {}\".format(index.shape, k_val.shape)\n",
    "        self.k[:, :, index] = k_val\n",
    "        self.v[:, :, index] = v_val\n",
    "\n",
    "        return self.k, self.v\n",
    "    \n",
    "\n",
    "class ModelKVCache(nn.Module):\n",
    "    def __init__(self, num_layers, max_batch_size, max_seq_len, hidden_size, n_heads, device='cpu', dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_heads = n_heads\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.cache = nn.ModuleList([])\n",
    "\n",
    "    def initialize_cache(self):\n",
    "        self.cache = nn.ModuleList([KVCache(self.max_batch_size, self.max_seq_len, self.hidden_size, self.n_heads, self.device, self.dtype) for _ in range(self.num_layers)])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.cache[index]\n",
    "    \n",
    "    def reset_cache(self):\n",
    "        self.cache = nn.ModuleList([])\n",
    "\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, hidden_size: int, n_heads: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.Wqkv = nn.Linear(hidden_size, 3 * hidden_size, bias=True)\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor, kv_cache: KVCache, index: torch.Tensor, rope: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.Wqkv(x)\n",
    "\n",
    "        q, k, v = qkv.split(C, dim=-1)\n",
    "\n",
    "        head_size = C // self.n_heads \n",
    "        q = q.view(B, T, self.n_heads, head_size)\n",
    "        k = k.view(B, T, self.n_heads, head_size)\n",
    "        v = v.view(B, T, self.n_heads, head_size)\n",
    "\n",
    "        q = apply_rope(q, rope)\n",
    "        k = apply_rope(k, rope)\n",
    "\n",
    "        k = k.transpose(1, 2)  # (B, nh, T, hs)\n",
    "        q = q.transpose(1, 2)  # (B, nh, T, hs)\n",
    "        v = v.transpose(1, 2)  # (B, nh, T, hs)\n",
    "\n",
    "\n",
    "        k, v = kv_cache.update(k, v, index)\n",
    "\n",
    "        context = F.scaled_dot_product_attention(q, k, v, attn_mask = mask, dropout_p=0.0)\n",
    "\n",
    "        context = context.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        out = self.out_proj(context)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, hidden_size: int, n_heads: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(hidden_size, eps=1e-5)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.mha = MHA(hidden_size, n_heads, dropout)\n",
    "        self.mlp = MLP(hidden_size, dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor, kv_cache: KVCache, index:torch.Tensor, rope:torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        residual = x\n",
    "        x = self.ln(x)\n",
    "        attn_out = self.mha(x, mask, kv_cache, index, rope)\n",
    "        attn_out = self.resid_dropout(attn_out)\n",
    "        ff = self.resid_dropout(self.mlp(x))\n",
    "\n",
    "        return residual + attn_out + ff\n",
    "\n",
    "\n",
    "class LMHead(nn.Module):\n",
    "    def __init__(self, hidden_size: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size, bias=True)\n",
    "        self.ln = nn.LayerNorm(hidden_size, eps=1e-5)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.ln(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Phi(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        n_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "        n_blocks: int = 24,\n",
    "        vocab_size: int = 51200,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, hidden_size, dropout)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(hidden_size, n_heads, dropout) for _ in range(n_blocks)]\n",
    "        )\n",
    "        self.lm_head = LMHead(hidden_size, vocab_size)\n",
    "\n",
    "        self.kv_cache = ModelKVCache(n_blocks, 1, 1024, hidden_size, n_heads)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor, index: torch.Tensor, rope: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x = block(x, kv_cache=self.kv_cache[i], mask=mask, index=index, rope=rope)\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "    \n",
    "    def init_cache(self):\n",
    "        self.kv_cache.initialize_cache()\n",
    "\n",
    "        ones = torch.ones((2048, 2048))\n",
    "        self.mask_cache = torch.tril(ones).unsqueeze(0)\n",
    "\n",
    "        self.rope_cache = build_rope_cache(\n",
    "            seq_len=self.hidden_size,\n",
    "            n_elem=32,\n",
    "        )\n",
    "\n",
    "\n",
    "    def generate(self, input_ids, max_length=1):\n",
    "\n",
    "        # input_ids (BS, seq_len)\n",
    "        bs, seq_len = input_ids.shape\n",
    "        out_seq_len = seq_len + max_length\n",
    "        out_vec = torch.empty((bs, out_seq_len))\n",
    "        out_vec[:, :seq_len] = input_ids\n",
    "\n",
    "        index = torch.arange(0, seq_len)\n",
    "        mask = self.mask_cache.index_select(2, index)\n",
    "        mask = mask[:, :seq_len, :seq_len]\n",
    "        print(mask.shape)\n",
    "        rope = self.rope_cache.index_select(0, index)\n",
    "        # process input_ids and get first output\n",
    "        output = self(input_ids, mask.bool(), index, rope)\n",
    "\n",
    "        return output\n",
    "\n",
    "# load model from model/phi-1.5/pytorch_model.bin\n",
    "model = Phi(hidden_size=2048, n_heads=32, dropout=0.0, n_blocks=24, vocab_size=51200)\n",
    "model.init_cache()\n",
    "\n",
    "# print number of params\n",
    "# print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "# 1418270720\n",
    "\n",
    "# load model from model/phi-1.5/pytorch_model.bin\n",
    "weights = torch.load(\"../model/phi-1.5/pytorch_model.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi(\n",
       "  (embedding): Embedding(\n",
       "    (embedding): Embedding(51200, 2048)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mha): MHA(\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (gelu): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mha): MHA(\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (gelu): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mha): MHA(\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (gelu): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mha): MHA(\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (gelu): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mha): MHA(\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (gelu): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mha): MHA(\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (gelu): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mha): MHA(\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (gelu): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mha): MHA(\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (gelu): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mha): MHA(\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (gelu): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mha): MHA(\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (gelu): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mha): MHA(\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (gelu): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mha): MHA(\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (gelu): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (12): Block(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mha): MHA(\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (gelu): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (13): Block(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mha): MHA(\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (gelu): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (14): Block(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mha): MHA(\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (gelu): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (15): Block(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mha): MHA(\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (gelu): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (16): Block(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mha): MHA(\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (gelu): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (17): Block(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mha): MHA(\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (gelu): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (18): Block(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mha): MHA(\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (gelu): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (19): Block(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mha): MHA(\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (gelu): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (20): Block(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mha): MHA(\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (gelu): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (21): Block(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mha): MHA(\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (gelu): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (22): Block(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mha): MHA(\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (gelu): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (23): Block(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mha): MHA(\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (gelu): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): LMHead(\n",
       "    (linear): Linear(in_features=2048, out_features=51200, bias=True)\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (kv_cache): ModelKVCache(\n",
       "    (cache): ModuleList(\n",
       "      (0-23): 24 x KVCache()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def map_keys(state_dict_keys):\n",
    "    key_mapping = {}\n",
    "\n",
    "    for key in state_dict_keys:\n",
    "        # Extract block number\n",
    "        block_num = (\n",
    "            int(key.split(\".\")[1]) - 1\n",
    "        )  # Adjusting for the difference in starting index\n",
    "\n",
    "        if \"wte\" in key:\n",
    "            mapped_key = key.replace(\"layers.0.wte\", \"embedding.embedding\")\n",
    "        else:\n",
    "            if block_num == 24:\n",
    "                mapped_key = key.replace(\"layers.25\", \"lm_head\")\n",
    "\n",
    "            # Map layer norm\n",
    "            elif \".ln.\" in key:\n",
    "                mapped_key = key.replace(\n",
    "                    f\"layers.{block_num + 1}.ln\", f\"blocks.{block_num}.ln\"\n",
    "                )\n",
    "\n",
    "            # Map multi-head attention / rotary embedding\n",
    "            elif \".mixer.\" in key:\n",
    "                if \"rotary_emb\" in key:\n",
    "                    mapped_key = key.replace(\n",
    "                        f\"layers.{block_num + 1}.mixer\", f\"blocks.{block_num}.mha\"\n",
    "                    )\n",
    "                else:\n",
    "                    mapped_key = key.replace(\n",
    "                        f\"layers.{block_num + 1}.mixer\", f\"blocks.{block_num}.mha\"\n",
    "                    )\n",
    "\n",
    "            # Map MLP layers\n",
    "            elif \".mlp.\" in key:\n",
    "                mapped_key = key.replace(\n",
    "                    f\"layers.{block_num + 1}.mlp.fc1\", f\"blocks.{block_num}.mlp.linear1\"\n",
    "                )\n",
    "                mapped_key = mapped_key.replace(\n",
    "                    f\"layers.{block_num + 1}.mlp.fc2\", f\"blocks.{block_num}.mlp.linear2\"\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected key: {key}\")\n",
    "\n",
    "        key_mapping[key] = mapped_key\n",
    "\n",
    "    return key_mapping\n",
    "\n",
    "\n",
    "key_mapping = map_keys(weights.keys())\n",
    "weights = {key_mapping[k]: v for k, v in weights.items()}\n",
    "\n",
    "# get rid of all keys that have inv_freq in them\n",
    "weights = {k: v for k, v in weights.items() if \"inv_freq\" not in k}\n",
    "\n",
    "model.load_state_dict(weights)\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixFormerSequentialForCausalLM(\n",
       "  (layers): Sequential(\n",
       "    (0): Embedding(\n",
       "      (wte): Embedding(51200, 2048)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (2): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (3): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (4): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (5): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (6): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (7): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (8): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (9): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (10): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (11): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (12): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (13): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (14): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (15): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (16): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (17): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (18): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (19): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (20): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (21): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (22): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (23): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (24): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (25): CausalLMHead(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Linear(in_features=2048, out_features=51200, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (loss): CausalLMLoss(\n",
       "    (loss_fct): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"../model/phi-1.5/\", trust_remote_code=True\n",
    ")\n",
    "\n",
    "reference_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 1024])\n",
      "torch.Size([1, 1024, 51200])\n",
      "torch.Size([1, 1024, 51200])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'atol' is an invalid keyword argument for print()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(output\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(reference_output\u001b[39m.\u001b[39mlogits\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 11\u001b[0m \u001b[39mprint\u001b[39;49m(torch\u001b[39m.\u001b[39;49mallclose(output, reference_output\u001b[39m.\u001b[39;49mlogits), atol\u001b[39m=\u001b[39;49m\u001b[39m1e-5\u001b[39;49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'atol' is an invalid keyword argument for print()"
     ]
    }
   ],
   "source": [
    "input_ids = torch.randint(0, 51200, (1, 1024))\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_ids)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reference_output = reference_model(input_ids)\n",
    "\n",
    "print(output.shape)\n",
    "print(reference_output.logits.shape)\n",
    "print(torch.allclose(output, reference_output.logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.allclose(output, reference_output.logits, atol=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.allclose(output, reference_output.logits))\n",
    "\n",
    "# print(output)\n",
    "\n",
    "# print(reference_output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embed_output = model.embedding(input_ids)\n",
    "reference_model_embed_output = reference_model.get_input_embeddings()(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.allclose(model_embed_output, reference_model_embed_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 1024, 64])\n",
      "torch.Size([1, 32, 1024, 64]) torch.Size([1, 32, 1024, 64])\n",
      "torch.Size([1, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "block1 = model.blocks[0].mha\n",
    "\n",
    "index = torch.arange(0, 1024)\n",
    "\n",
    "mask = model.mask_cache.index_select(2, index)\n",
    "mask = mask[:, :1024, :1024]\n",
    "\n",
    "block1_output = block1(model_embed_output, mask.bool(), model.kv_cache[0], index, model.rope_cache)\n",
    "\n",
    "reference_block1 = reference_model.layers[1].mixer\n",
    "\n",
    "reference_block1_output = reference_block1(reference_model_embed_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 2048])\n",
      "torch.Size([1, 1024, 2048])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# print(torch.allclose(block1_output, reference_block1_output))\n",
    "# print(block1_output.shape)\n",
    "# print(reference_block1_output.shape)\n",
    "# print(block1_output[0].shape)\n",
    "# print(reference_block1_output[:,:,0].shape)\n",
    "# block1_output = block1_output[0].transpose(1, 2)\n",
    "# print(block1_output.shape)\n",
    "\n",
    "print(block1_output.shape)\n",
    "print(reference_block1_output.shape)\n",
    "\n",
    "print(torch.allclose(block1_output, reference_block1_output, atol=1e-8) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0388, -0.1463,  0.1651,  ..., -0.0249,  0.0591, -0.0381],\n",
      "         [-0.0160, -0.1241,  0.1468,  ..., -0.0386, -0.0120, -0.0438],\n",
      "         [-0.0244, -0.1089,  0.1391,  ..., -0.0567, -0.0083, -0.0171],\n",
      "         ...,\n",
      "         [ 0.0270, -0.0092,  0.0690,  ..., -0.0085, -0.0140, -0.0169],\n",
      "         [ 0.0270, -0.0092,  0.0689,  ..., -0.0086, -0.0140, -0.0167],\n",
      "         [ 0.0269, -0.0092,  0.0689,  ..., -0.0085, -0.0140, -0.0168]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.0388, -0.1463,  0.1651,  ..., -0.0249,  0.0591, -0.0381],\n",
      "         [-0.0160, -0.1241,  0.1468,  ..., -0.0386, -0.0120, -0.0438],\n",
      "         [-0.0244, -0.1089,  0.1391,  ..., -0.0567, -0.0083, -0.0171],\n",
      "         ...,\n",
      "         [ 0.0270, -0.0092,  0.0690,  ..., -0.0085, -0.0140, -0.0169],\n",
      "         [ 0.0270, -0.0092,  0.0689,  ..., -0.0086, -0.0140, -0.0167],\n",
      "         [ 0.0269, -0.0092,  0.0689,  ..., -0.0085, -0.0140, -0.0168]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(block1_output)\n",
    "print(reference_block1_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n"
     ]
    }
   ],
   "source": [
    "for i in range(1024):\n",
    "    if(torch.allclose(block1_output[:,i], reference_block1_output[:,i], atol=1e-3) ):\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
